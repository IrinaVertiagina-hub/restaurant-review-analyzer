import streamlit as st
import pickle
import pandas as pd
from scipy.sparse import hstack
import re
import matplotlib.pyplot as plt
import seaborn as sns

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Restaurant Review Analyzer",
    page_icon="üçΩÔ∏è",
    layout="wide"
)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏ (–∫—ç—à–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –∑–∞–≥—Ä—É–∂–∞—Ç—å –æ–¥–∏–Ω —Ä–∞–∑)
@st.cache_resource
def load_models():
    with open('models/tfidf_vectorizer.pkl', 'rb') as f:
        vectorizer = pickle.load(f)
    
    with open('models/xgboost_model.pkl', 'rb') as f:
        model = pickle.load(f)
    
    with open('models/aspect_keywords.pkl', 'rb') as f:
        aspect_keywords = pickle.load(f)
    
    with open('models/model_metadata.pkl', 'rb') as f:
        metadata = pickle.load(f)
    
    return vectorizer, model, aspect_keywords, metadata

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
def extract_features(text):
    features = {
        'text_length': len(text),
        'word_count': len(text.split()),
        'exclamation_count': text.count('!'),
        'question_count': text.count('?'),
        'capital_ratio': sum(1 for c in text if c.isupper()) / len(text) if len(text) > 0 else 0,
        'avg_word_length': len(text) / len(text.split()) if len(text.split()) > 0 else 0
    }
    return pd.DataFrame([features])

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ–±—â–µ–≥–æ sentiment
def predict_sentiment(text, vectorizer, model):
    # TF-IDF
    text_vector = vectorizer.transform([text])
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    extra_features = extract_features(text)
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º
    combined = hstack([text_vector, extra_features.values])
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    prediction = model.predict(combined)[0]
    probabilities = model.predict_proba(combined)[0]
    
    return prediction, probabilities

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π —Å –∞—Å–ø–µ–∫—Ç–æ–º
# def extract_aspect_sentences(text, aspect_words):
#     sentences = re.split(r'[.!?]+', text.lower())
#     relevant_sentences = []
#     for sentence in sentences:
#         if any(word in sentence for word in aspect_words):
#             relevant_sentences.append(sentence.strip())
#     return relevant_sentences
def extract_aspect_sentences(text, aspect_words):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –∞—Å–ø–µ–∫—Ç–∞, –∏–∑–±–µ–≥–∞—è –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–ª–æ–∂–Ω—ã—Ö –º–Ω–µ–Ω–∏–π"""
    text_lower = text.lower()
    sentences = re.split(r'[.!?]+', text_lower)
    relevant = []
    
    # –°–ª–æ–≤–∞-—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–∏ (but, however, although)
    separators = ['but', 'however', 'although', 'though', 'yet']
    
    for sentence in sentences:
        words = sentence.split()
        
        for i, word in enumerate(words):
            if any(aspect_word in word for aspect_word in aspect_words):
                # –ù–∞–π–¥—ë–º –±–ª–∏–∂–∞–π—à–∏–π separator –ü–ï–†–ï–î –∞—Å–ø–µ–∫—Ç–æ–º
                separator_idx = -1
                for j in range(i-1, -1, -1):
                    if words[j] in separators:
                        separator_idx = j
                        break
                
                # –ù–∞—á–∏–Ω–∞–µ–º –ü–û–°–õ–ï separator (–∏–ª–∏ —Å –Ω–∞—á–∞–ª–∞ –µ—Å–ª–∏ separator –Ω–µ—Ç)
                start = max(0, separator_idx + 1)
                
                # –ë–µ—Ä—ë–º –¥–æ –∫–æ–Ω—Ü–∞ –∏–ª–∏ –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ separator
                end = len(words)
                for j in range(i+1, len(words)):
                    if words[j] in separators:
                        end = j
                        break
                
                context = ' '.join(words[start:end])
                relevant.append(context.strip())
                break
    
    return relevant

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ sentiment –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å–ø–µ–∫—Ç–∞ (–∏–∑ Jupyter cell 19)
def analyze_aspect_sentiment(text, aspect_words, vectorizer, model):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç sentiment –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∞—Å–ø–µ–∫—Ç–∞"""
    sentences = extract_aspect_sentences(text, aspect_words)
    
    if not sentences:
        return None  # –∞—Å–ø–µ–∫—Ç –Ω–µ —É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ–± –∞—Å–ø–µ–∫—Ç–µ
    aspect_text = ' '.join(sentences)
    
    # 1. TF-IDF –≤–µ–∫—Ç–æ—Ä
    aspect_tfidf = vectorizer.transform([aspect_text])
    
    # 2. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
    extra_features = pd.DataFrame({
        'text_length': [len(aspect_text)],
        'word_count': [len(aspect_text.split())],
        'exclamation_count': [aspect_text.count('!')],
        'question_count': [aspect_text.count('?')],
        'capital_ratio': [sum(1 for c in aspect_text if c.isupper()) / len(aspect_text) if len(aspect_text) > 0 else 0],
        'avg_word_length': [len(aspect_text) / len(aspect_text.split()) if len(aspect_text.split()) > 0 else 0]
    })
    
    # 3. –û–±—ä–µ–¥–∏–Ω—è–µ–º
    aspect_vector = hstack([aspect_tfidf, extra_features.values])
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º sentiment
    sentiment = model.predict(aspect_vector)[0]
    sentiment_proba = model.predict_proba(aspect_vector)[0]
    
    return {
        'sentiment': sentiment,
        'confidence': sentiment_proba[sentiment],
        'text_sample': sentences[0] if sentences else ''
    }

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Å–µ—Ö –∞—Å–ø–µ–∫—Ç–æ–≤
def analyze_aspects(text, vectorizer, model, aspect_keywords):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ –∞—Å–ø–µ–∫—Ç—ã –æ—Ç–∑—ã–≤–∞"""
    results = {}
    
    for aspect_name, keywords in aspect_keywords.items():
        result = analyze_aspect_sentiment(text, keywords, vectorizer, model)
        
        if result:
            results[aspect_name] = {
                'sentiment': 'Positive' if result['sentiment'] == 1 else 'Negative',
                'confidence': result['confidence'],
                'sample': result['text_sample']
            }
    
    return results

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
vectorizer, model, aspect_keywords, metadata = load_models()

# === UI ===

st.title("üçΩÔ∏è Restaurant Review Analyzer")
st.markdown("### Analyze restaurant reviews with AI-powered sentiment analysis")

# Sidebar —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –º–æ–¥–µ–ª–∏
with st.sidebar:
    st.header("üìä Model Information")
    st.metric("Accuracy", f"{metadata['baseline_accuracy']*100:.1f}%")
    st.metric("Training Samples", f"{metadata['training_samples']:,}")
    st.metric("Features", metadata['features'])
    st.markdown("---")
    st.markdown("**Aspects Analyzed:**")
    st.markdown("üçï Food ‚Ä¢ üë• Service ‚Ä¢ üè† Ambiance ‚Ä¢ üí∞ Price")

# –ì–ª–∞–≤–Ω–∞—è —Å–µ–∫—Ü–∏—è
tab1, tab2 = st.tabs(["üìù Analyze Review", "‚ÑπÔ∏è About"])

with tab1:
    st.markdown("### Enter a restaurant review:")
    
    # –ü—Ä–∏–º–µ—Ä—ã –æ—Ç–∑—ã–≤–æ–≤
    example_reviews = {
        "Positive Example": "The food was absolutely delicious! Great service and wonderful atmosphere. Highly recommend!",
        "Negative Example": "Terrible service, waited 45 minutes. Food was cold and overpriced. Never coming back.",
        "Mixed Example": "Food was amazing but service was really slow. Prices are reasonable though."
    }
    
    selected_example = st.selectbox("Or select an example:", ["Custom"] + list(example_reviews.keys()))
    
    if selected_example == "Custom":
        user_review = st.text_area("Review text:", height=150, placeholder="Type or paste a restaurant review here...")
    else:
        user_review = st.text_area("Review text:", value=example_reviews[selected_example], height=150)
    
    if st.button("üîç Analyze Review", type="primary"):
        if user_review:
            with st.spinner("Analyzing..."):
                # –û–±—â–∏–π sentiment
                prediction, probabilities = predict_sentiment(user_review, vectorizer, model)
                
                # –†–µ–∑—É–ª—å—Ç–∞—Ç
                col1, col2 = st.columns([2, 1])
                
                with col1:
                    st.markdown("### Overall Sentiment")
                    if prediction == 1:
                        st.success(f"‚úÖ **POSITIVE** ({probabilities[1]*100:.1f}% confidence)")
                    else:
                        st.error(f"‚ùå **NEGATIVE** ({probabilities[0]*100:.1f}% confidence)")
                
                with col2:
                    # –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞
                    fig, ax = plt.subplots(figsize=(3, 3))
                    colors = ['#90EE90' if prediction == 1 else '#FFB6C6', 
                             '#FFB6C6' if prediction == 1 else '#90EE90']
                    ax.pie(probabilities, labels=['Negative', 'Positive'], 
                           autopct='%1.1f%%', colors=colors, startangle=90)
                    ax.set_title('Confidence')
                    st.pyplot(fig)
                
                # –ê–Ω–∞–ª–∏–∑ –ø–æ –∞—Å–ø–µ–∫—Ç–∞–º
                st.markdown("---")
                st.markdown("### üìä Aspect-Based Analysis")
                
                aspect_results = analyze_aspects(user_review, vectorizer, model, aspect_keywords)
                
                if aspect_results:
                    cols = st.columns(len(aspect_results))
                    
                    for idx, (aspect, result) in enumerate(aspect_results.items()):
                        with cols[idx]:
                            emoji = {'food': 'üçï', 'service': 'üë•', 'ambiance': 'üè†', 'price': 'üí∞'}
                            st.markdown(f"**{emoji.get(aspect, 'üìå')} {aspect.capitalize()}**")
                            
                            if result['sentiment'] == 'Positive':
                                st.success(f"‚úÖ Positive")
                            else:
                                st.error(f"‚ùå Negative")
                            
                            st.caption(f"{result['confidence']*100:.0f}% confident")
                            with st.expander("View excerpt"):
                                st.write(f"_{result['sample'][:100]}..._")
                else:
                    st.info("No specific aspects detected in this review.")
        else:
            st.warning("‚ö†Ô∏è Please enter a review to analyze.")

with tab2:
    st.markdown("""
    ### About This Project
    
    This AI-powered tool analyzes restaurant reviews using Machine Learning to provide:
    
    - **Overall Sentiment**: Determines if a review is positive or negative
    - **Aspect-Based Analysis**: Breaks down sentiment by Food, Service, Ambiance, and Price
    - **Confidence Scores**: Shows how certain the model is about its predictions
    
    #### Model Performance
    - **Accuracy**: 95.5% on 17,728 test reviews
    - **Training Data**: 70,910 Yelp restaurant reviews
    - **Algorithm**: Logistic Regression with TF-IDF features
    
    #### Key Insights from Analysis
    - üìä Price and Service are the most common complaint areas (22% and 21.7% negative)
    - üèÜ Ambiance receives the most positive feedback (88.4% positive)
    - üçï Food quality has 82% positive sentiment overall
    
    #### Model Limitations
    - Works best with complete, detailed reviews (50+ words)
    - May struggle with very short phrases or strong slang terms
    - Aspect analysis extracts relevant sentences but may be less accurate for brief mentions
    
    ---
    *Built with Python, scikit-learn, and Streamlit*
    """)

# Footer
st.markdown("---")
st.markdown("*Restaurant Review Analyzer ‚Ä¢ Built by Irina Vertiagina*")



